---
title: "Long-term trend calculations"
output: 
  html_document:
    toc: true
    toc_float: true
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      error = TRUE,
                      fig.width = 7,
                      fig.height = 5)
```

```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(lubridate)
library(mgcv)
library(gratia)
library(gt)
library(gtsummary)


source(here::here("helper_files", "definitions.R"))
source(here::here("helper_files", "functions.R"))

load(here::here("Data", "QAQCd_monthly_byType", "SWMP_monthlyWQ.RData"))
load(here::here("Data", "QAQCd_monthly_byType", "SWMP_monthlyNUT.RData"))
load(here::here("Data", "QAQCd_monthly_byType", "SWMP_monthlyMET.RData"))
```


# Explanation of trend calculations  

## Station selection  

Only SWMP stations that started collecting data before 2013, and that were active as of the end of 2022, are represented here. Additionally, only stations that exist for BOTH WQ and NUT data types are represented.  

## Data point inclusion  

See the data processing README files for explanations on data point inclusion/exclusion. Generally, data points flagged as either suspect or rejected were excluded from these analyses.  

## Monthly aggregation  

Again, see the data processing README files. Additionally, see the `Outputs/calculated_trends/data_dictionary_trend_parameters.csv` file for parameters analyzed and transformations made. For WQ and MET, we are generally working with monthly median values. For NUTs, we are working with averaged replicates for each month.  

## Trend calculation  

We have generally used GAMs (generalized additive models) to calculate trends. A seasonal term is included, with 12 knots if possible and the number of months represented in the data frame otherwise (e.g. stations where sondes are removed part of the year due to ice). Autocorrelation of residuals is automatically checked for and if present, the model is re-run to account for the autocorrelation. The reported trend in the outputs is the LINEAR trend through time (per year) of the parameter. To account for censoring, autocorrelation, and seasonality, we used `mgcv::bam`.    

For NUT parameters: values were log10 transformed, and marked as censored (e.g. below the minimum detection limit, or MDL) or uncensored. This created a response matrix that was used in `mgcv::bam` with `family = cnorm()` to account for censoring.  

For WQ monthly medians, we also used `mgcv::bam` for consistency in outputs. There is no censoring in these parameters, so we used `family = gaussian()`.  

For WQ proportion of DO below 2 and 5: These calculations were made before monthly aggregation - each valid 15-minute data point was marked TRUE/FALSE for below 2 and 5, respectively (in separate columns). During monthly aggregation, the total TRUE for each month was divided by the total number of valid DO points for the month, leading to a proportion per month. Trends were again calculated in `mgcv::bam()` with a seasonal term and an autocorrelation term if necessary. Because this response is a proportion, we used `family = betar()`. The `eps` option, which adjusts exact 0s and 1s, was set to 1/10th of the minimum number of readings per month (1/27900).     

For MET parameters, we again used the `bam` code written for water quality, as the properties are similar. Performed tests on 3 parameters: monthly median air temperature (C), monthly total precipitation (mm), and the monthly median of daily total PAR. Monthly precipitation was square-root transformed before analysis (this produced the best residual diagnostics on 5 stations explored).    


## Seasonal trends  

This part is simpler and rougher: we have not accounted for autocorrelation or any "wiggliness" in the data. We simply split data into four seasons: Winter (Jan, Feb, Mar); Spring (Apr, May, Jun); Summer (Jul, Aug, Sep); Fall (Oct, Nov, Dec) and calculated a linear trend. For WQ medians, we used the simple `lm()` function. Nutrients still used `mgcv::bam()` to account for censoring (`family = cnorm()`), and DO proportions also used `mgcv::bam()` with `family = betar()`.  


p-values have NOT been adjusted from any of these analyses, so be wary about declaring any individual trend significant based on its reported p-value. Be especially wary about seasonal p-values, as autocorrelation is not accounted for.  



# Setup  

Subset wq and nut data frames; create param grids.  

**Prioritized parameters**:
*Responses*:  NO3/2, NH4, PO4, Chla, DO, DIN:DIP, proportion of DO data below 2 mg/L; proportion of DO data below 5 mg/L
*Driver variables*: Temp (water and air), Precipitation, Sal, Cond, Stratification, Turbidity, PAR  


```{r}
dat_wq <- wq |> 
  filter(station %in% paste0(stns_wq_nut_d10, "wq")) |> 
  mutate(do_proportion_below2 = round(doLessThan2_total / doLessThan2_nValid, 4),
         do_proportion_below5 = round(doLessThan5_total / doLessThan5_nValid, 4)) |> 
  select(station, year, month, 
         do_pct_median, do_mgl_median,
         temp_median, spcond_median, 
         sal_median, turb_median,
         do_proportion_below2,
         do_proportion_below5)

# may need to treat DO proportions differently from others - not gaussian, but beta

dat_met <- met |> 
  filter(station %in% paste0(stns_met_d10, "met")) |> 
  select(station, year, month, 
         atemp_median,
         totprcp_total,
         dailyPAR_median) |> 
  mutate(sqrt_precp_total = sqrt(totprcp_total))  # SQUARE-ROOT TRANSFORM

dat_nut <- nut |> 
  filter(station %in% paste0(stns_wq_nut_d10, "nut")) |> 
  select(station, year, month, 
         chla_n, po4f,
         nh4f, no23f,
         chla_n_cens, po4f_cens,
         nh4f_cens, no23f_cens) |> 
  mutate(DIN_to_DIP = (nh4f + no23f) / po4f,
         DIN_to_DIP_cens = case_when(nh4f_cens + no23f_cens + po4f_cens == 0 ~ 0,
                                     .default = 1),
         DIN_to_DIP_censNum = case_when(nh4f_cens + no23f_cens > 0 ~ 1,
                                        .default = 0),
         DIN_to_DIP_censDenom = case_when(po4f_cens == 1 ~ 1,
                                          .default = 0),
         DIN_to_DIP_censBoth = case_when(DIN_to_DIP_censNum + DIN_to_DIP_censDenom == 2 ~ 1,
                                         .default = 0))

# DIN_to_DIP may also need to be treated in a non-gaussian way

```

```{r}
# avoiding all proportion/ratio data at this point
wq_grid <- expand.grid(station = unique(dat_wq$station), 
                       param = names(dat_wq[4:9]))
nut_grid <- expand.grid(station = unique(dat_nut$station), 
                       param = names(dat_nut[4:7]))
met_grid <- expand.grid(station = unique(dat_met$station), 
                       param = names(dat_met[c(4, 6, 7)]))

# proportions
do_grid <- expand.grid(station = unique(dat_wq$station), 
                       param = names(dat_wq[10:11]))

# seasonal definitions
seas_defs <- data.frame(month = 1:12,
                        season = rep(c("Winter", "Spring", "Summer", "Fall"), each = 3))
```

# WQ  

## trend calcs  

Attempting to use 12 knots for a seasonal smooth. This not possible at all stations (many are not sampled during winter) so at the ones where 12 knots causes a model error, I am allowing the model to choose the number of knots for me by setting it back to the default of k = -1. (This is all happening in the functions `run_bam_wq()` and `run_bam_nut()`).  

```{r}
# wq_grid <- wq_grid[1:3, ]

# lksolwq too short time series for winter trends
# i = 50

# set up outputs
trends_out <- list()


# run the loop
for(i in 1:nrow(wq_grid)){
  stn <- as.character(wq_grid$station[i])
  param <- as.character(wq_grid$param[i])
  
  # subset to that stn/param combo
  tmp <- subset_df("wq", dat_wq, stn, param)
  
  # capture some info about the station and time series:
  ts_chars <- data.frame(ts_start = min(tmp$dec_date, na.rm = TRUE),
                         ts_end = max(tmp$dec_date, na.rm = TRUE),
                         ts_length = max(tmp$dec_date, na.rm = TRUE) - min(tmp$dec_date, na.rm = TRUE),
                         n_points = sum(!is.na(tmp$value)),
                         overall_median = median(tmp$value, na.rm = TRUE))
  
  
  # run the model
  #########
  
  # if the below statement is true, there was an error in the model
  # and we need to do something, and move to the next item in the loop
  # if it was false, we can move on
  if(inherits(try(run_bam_wq(tmp, k = 12), silent = TRUE), "try-error")){
    # pick our own k:
    knew <- tmp |>
      summarize(.by = month,
                nVals = sum(!is.na(value))) |>
      filter(nVals >= 1) |>
      nrow()
    
    if(inherits(try(run_bam_wq(tmp, k = knew), silent = TRUE), "try-error")){
      # if that didn't work, make a blank row in the output:
      out <- data.frame(station = stn,
                        parameter = param,
                        model_error = TRUE)
      # if it did work:
    } else {
      bam_out <- run_bam_wq(tmp, k = knew)
    }
    
  } else {
    # run model
    bam_out <- run_bam_wq(tmp, k = 12)
  }
  
  
  # even if it worked, if the time series is too short, remove it
  if(ts_chars$ts_length < 9){
    out <- data.frame(station = stn,
                      parameter = param,
                      model_error = TRUE)       
    if(exists("bam_out")){
      rm(bam_out)
    }
  }
  
  # save and tidy the output
  #####
  if(exists("bam_out")){
    
    # save the entire bam object, including autocorrelation info
    file_out <- here::here("Outputs", "calculated_trends", "bam_outputs",
                           paste0(stn, "_", param, "_bam.RData"))
    save(bam_out, file = file_out)
    
    out <- tidy_bam_output(bam_obj = bam_out)
    rm(bam_out)
  }
  
  # bind to output list
  out <- bind_cols(out, ts_chars)
  
  
  # break out trends by season
  # only if there's enough data
  if(ts_chars$ts_length >= 9){
    
    tmp <- left_join(tmp, seas_defs)
    season_vec <- unique(tmp$season)
    seasonals <- list()
    
    
    # there also needs to be enough data within each season
    for(j in seq_along(season_vec)){
      tmp_seas <- filter(tmp, season == season_vec[j])
      
      nYears <- tmp_seas |> 
        summarize(.by = year,
                  values = sum(!is.na(value))) |> 
        filter(values >= 1) |> 
        nrow()
      
      if(nYears >= 7){
        # if you can run an lm for a season, do it
        if(!inherits(try(lm(value ~ dec_date, data = tmp_seas), 
                         silent = TRUE), "try-error")){
          mod <- lm(value ~ dec_date, 
                    data = tmp_seas)
          seas_tbl <- broom::tidy(mod) |> 
            filter(term == "dec_date") |> 
            mutate(station = stn,
                   parameter= param,
                   season = season_vec[j]) |> 
            select(-term, -statistic) |> 
            relocate(station, parameter, season)
          
          seasonals[[j]] <- seas_tbl
        } else {
          seasonals[[j]] <- data.frame(station = stn,
                                       parameter = param,
                                       season = season_vec[j],
                                       estimate = NA_real_,
                                       p.value = NA_real_)
        }
        
        # if there aren't at least 7 years 
      } else {
        seasonals[[j]] <- data.frame(station = stn,
                                     parameter = param,
                                     season = season_vec[j],
                                     estimate = NA_real_,
                                     p.value = NA_real_)
      }
      
      
    }
    seasonal_trends <- bind_rows(seasonals) |> 
      pivot_wider(names_from = season,
                  values_from = c(estimate, p.value, std.error),
                  names_glue = "{season}_{.value}")
    
    out <- left_join(out, seasonal_trends)
  }
  
  trends_out[[i]] <- out
  rm(out)
}

```

```{r}
trends_df <- dplyr::bind_rows(trends_out) |> 
  mutate(sig_trend = case_when(p.value <= 0.05 ~ "yes",
                               is.na(p.value) ~ NA_character_,
                               .default = "no"),
         sig_seasonality = case_when(Seas_p.val <= 0.05 ~ "yes",
                                     is.na(Seas_p.val) ~ NA_character_,
                                     .default = "no"),
         sig_autocorr = case_when(model_refit == TRUE ~ "yes",
                                  is.na(model_refit) ~ NA_character_,
                                  .default = "no"))
```


Originally errored on grbgbwq, with this error: "more knots than unique data values is not allowed". This seems to have been due to lack of year-round sampling and has been fixed.  


Anything that didn't run anyway?  

```{r}
trends_df |> 
  filter(model_error == TRUE) |> 
  select(station, parameter) |> 
  summarize(.by = station,
            parameters = paste(unique(parameter), collapse = "; ")) |> 
  arrange(station) |> 
  gt(caption = "WQ stations where the model did not run")
```


## DO proportion trends  

Using a beta distribution for this. Otherwise same process as above.  

This originally was much slower than gaussian, and gave a lot of warnings:

```
In object$family$saturated.ll(y, wts, object$family$getTheta(TRUE)) :
  saturated likelihood may be inaccurate
```

After googling, I found this warning can be caused by a lot of 0s and/or 1s. One or the other can be handled pretty well by increasing the `eps`, which affects how 0s and 1s are handled (beta family cannot handle exact 0s or 1s). My computer's default, `.Machine$double.eps*100` is on the order of 10^-14. Because the highest number of readings we might have in a month would be 4/hr * 24 h/day * 31 days/month = 2976, one reading different from the others would be a proportion of 1/2976 = 0.000336. So we can set the eps to be lower than this, but still much higher than 10^-14. I will choose 10% of that proportion, 1/29760 (so 0 will be adjusted to 0.0000336 and 1 will be adjusted to 0.9999664). This is indeed much faster, though it doesn't get rid of all the warnings. I have adjusted it in the `run_bam_wqBeta()` function.  

```{r}
# set up outputs
trends_out <- list()


# run the loop
for(i in 1:nrow(do_grid)){
  stn <- as.character(do_grid$station[i])
  param <- as.character(do_grid$param[i])
  
  # subset to that stn/param combo
  tmp <- subset_df("wq", dat_wq, stn, param)
  
  # capture some info about the station and time series:
  ts_chars <- data.frame(ts_start = min(tmp$dec_date, na.rm = TRUE),
                         ts_end = max(tmp$dec_date, na.rm = TRUE),
                         ts_length = max(tmp$dec_date, na.rm = TRUE) - min(tmp$dec_date, na.rm = TRUE),
                         n_points = sum(!is.na(tmp$value)),
                         overall_median = median(tmp$value, na.rm = TRUE))
  
  # if the below statement is true, there was an error in the model
  # and we need to do something, and move to the next item in the loop
  # if it was false, we can move on
  if(inherits(try(run_bam_wqBeta(tmp, k = 12), silent = TRUE), "try-error")){
    
    # pick our own k:
    knew <- tmp |> 
      summarize(.by = month,
                nVals = sum(!is.na(value))) |> 
      filter(nVals >= 1) |> 
      nrow()
    
    if(inherits(try(run_bam_wqBeta(tmp, k = knew), silent = TRUE), "try-error")){
      
      # if that didn't work, make a blank row in the output:
      out <- data.frame(station = stn,
                        parameter = param,
                        model_error = TRUE)  
      
      # if it did work:
    } else {
      bam_out <- run_bam_wqBeta(tmp, k = knew)
    }
    
    
  } else {
    # run model
    bam_out <- run_bam_wqBeta(tmp, k = 12)
  }
  
    # even if it worked, if the time series is too short, remove it
  if(ts_chars$ts_length < 9){
    out <- data.frame(station = stn,
                      parameter = param,
                      model_error = TRUE)       
    if(exists("bam_out")){
      rm(bam_out)
    }
  }
  
  # do all the things to whichever bam_out we got
  # assuming we got one
  
  if(exists("bam_out")){
    
    # save the entire bam object, including autocorrelation info
    file_out <- here::here("Outputs", "calculated_trends", "bam_outputs",
                           paste0(stn, "_", param, "_bam.RData"))
    save(bam_out, file = file_out)
    
    out <- tidy_bam_output(bam_obj = bam_out)
    rm(bam_out)
    }
  
  out <- bind_cols(out, ts_chars)
  
  
  # break out trends by season
  # only if there's enough data
  if(ts_chars$ts_length >= 9){
    
    tmp <- left_join(tmp, seas_defs)
    season_vec <- unique(tmp$season)
    seasonals <- list()
    
    for(j in seq_along(season_vec)){
      tmp_seas <- filter(tmp, season == season_vec[j])
      
      # if plain gam for a season works, run it
      if(!inherits(try(gam(value ~ dec_date, family = betar(), data = tmp_seas),
                       silent = TRUE), "try-error")){
        
        mod <- gam(value ~ dec_date,
                   family = betar(),
                   data = tmp_seas)
        seas_tbl <- gtsummary::tidy_gam(mod) |> 
          filter(term == "dec_date") |> 
          mutate(station = stn,
                 parameter= param,
                 season = season_vec[j]) |> 
          select(-term, -statistic, -parametric) |> 
          relocate(station, parameter, season)
        
        seasonals[[j]] <- seas_tbl
        
      } else {
        
        seasonals[[j]] <- data.frame(station = stn,
                                     parameter = param,
                                     season = season_vec[j],
                                     estimate = NA_real_,
                                     p.value = NA_real_)
      }
      
      
    }
    seasonal_trends <- bind_rows(seasonals) |> 
      pivot_wider(names_from = season,
                  values_from = c(estimate, p.value, std.error),
                  names_glue = "{season}_{.value}")
    
    out <- left_join(out, seasonal_trends)
  }
  
  trends_out[[i]] <- out
  rm(out)
}

```

```{r}
trends_df_do <- dplyr::bind_rows(trends_out) |> 
  mutate(sig_trend = case_when(p.value <= 0.05 ~ "yes",
                               is.na(p.value) ~ NA_character_,
                               .default = "no"),
         sig_seasonality = case_when(Seas_p.val <= 0.05 ~ "yes",
                                     is.na(Seas_p.val) ~ NA_character_,
                                     .default = "no"),
         sig_autocorr = case_when(model_refit == TRUE ~ "yes",
                                  is.na(model_refit) ~ NA_character_,
                                  .default = "no"))
```


```{r}
trends_df_do |> 
  filter(model_error == TRUE) |> 
  select(station, parameter) |> 
  summarize(.by = station,
            parameters = paste(unique(parameter), collapse = "; ")) |> 
  arrange(station) |> 
  gt(caption = "WQ stations where the model did not run")
```


### bind to other wq trends  

```{r}
trends_df <- bind_rows(trends_df, trends_df_do)
```

***
***

# MET  

Should use all the same syntax as WQ.  

## trend calcs  

Attempting to use 12 knots for a seasonal smooth. This not possible at all stations (many are not sampled during winter) so at the ones where 12 knots causes a model error, I am allowing the model to choose the number of knots for me by setting it back to the default of k = -1. (This is all happening in the functions `run_bam_wq()` and `run_bam_nut()`).  

```{r}
# set up outputs
trends_out <- list()


# run the loop
for(i in 1:nrow(met_grid)){
  stn <- as.character(met_grid$station[i])
  param <- as.character(met_grid$param[i])
  
  # subset to that stn/param combo
  tmp <- subset_df("wq", dat_met, stn, param)
  
  # capture some info about the station and time series:
  ts_chars <- data.frame(ts_start = min(tmp$dec_date, na.rm = TRUE),
                         ts_end = max(tmp$dec_date, na.rm = TRUE),
                         ts_length = max(tmp$dec_date, na.rm = TRUE) - min(tmp$dec_date, na.rm = TRUE),
                         n_points = sum(!is.na(tmp$value)),
                         overall_median = median(tmp$value, na.rm = TRUE))
  
  
  # run the model
  #########
  
  # if the below statement is true, there was an error in the model
  # and we need to do something, and move to the next item in the loop
  # if it was false, we can move on
  if(inherits(try(run_bam_wq(tmp, k = 12), silent = TRUE), "try-error")){
    # pick our own k:
    knew <- tmp |>
      summarize(.by = month,
                nVals = sum(!is.na(value))) |>
      filter(nVals >= 1) |>
      nrow()

    if(inherits(try(run_bam_wq(tmp, k = knew), silent = TRUE), "try-error")){
      # if that didn't work, make a blank row in the output:
      out <- data.frame(station = stn,
                        parameter = param,
                        model_error = TRUE)
     # if it did work:
    } else {
      bam_out <- run_bam_wq(tmp, k = knew)
    }

  } else {
    # run model
    bam_out <- run_bam_wq(tmp, k = 12)
  }
  

    # even if it worked, if the time series is too short, remove it
  if(ts_chars$ts_length < 9){
    out <- data.frame(station = stn,
                      parameter = param,
                      model_error = TRUE)       
    if(exists("bam_out")){
      rm(bam_out)
    }
  }
  
  # save and tidy the output
  #####
  if(exists("bam_out")){
    
    # save the entire bam object, including autocorrelation info
    file_out <- here::here("Outputs", "calculated_trends", "bam_outputs",
                           paste0(stn, "_", param, "_bam.RData"))
    save(bam_out, file = file_out)
    
    out <- tidy_bam_output(bam_obj = bam_out)
    rm(bam_out)
  }
  
  # bind to output list
  out <- bind_cols(out, ts_chars)
  
  
  # break out trends by season
  # only if there's enough data
  if(ts_chars$ts_length >= 9){
    
    tmp <- left_join(tmp, seas_defs)
    season_vec <- unique(tmp$season)
    seasonals <- list()
    
    for(j in seq_along(season_vec)){
      tmp_seas <- filter(tmp, season == season_vec[j])
      
      # if you can run an lm for a season, do it
      if(!inherits(try(lm(value ~ dec_date, data = tmp_seas), 
                       silent = TRUE), "try-error")){
        mod <- lm(value ~ dec_date, 
                  data = tmp_seas)
        seas_tbl <- broom::tidy(mod) |> 
          filter(term == "dec_date") |> 
          mutate(station = stn,
                 parameter= param,
                 season = season_vec[j]) |> 
          select(-term, -statistic) |> 
          relocate(station, parameter, season)
        
        seasonals[[j]] <- seas_tbl
      } else {
        seasonals[[j]] <- data.frame(station = stn,
                                     parameter = param,
                                     season = season_vec[j],
                                     estimate = NA_real_,
                                     p.value = NA_real_)
      }
      
    }
    seasonal_trends <- bind_rows(seasonals) |> 
      pivot_wider(names_from = season,
                  values_from = c(estimate, p.value, std.error),
                  names_glue = "{season}_{.value}")
    
    out <- left_join(out, seasonal_trends)
  }
  
  trends_out[[i]] <- out
  rm(out)
}

```

```{r}
trends_met <- dplyr::bind_rows(trends_out) |> 
  mutate(sig_trend = case_when(p.value <= 0.05 ~ "yes",
                               is.na(p.value) ~ NA_character_,
                               .default = "no"),
         sig_seasonality = case_when(Seas_p.val <= 0.05 ~ "yes",
                                     is.na(Seas_p.val) ~ NA_character_,
                                     .default = "no"),
         sig_autocorr = case_when(model_refit == TRUE ~ "yes",
                                  is.na(model_refit) ~ NA_character_,
                                  .default = "no"))
```


Anything that didn't run?  

```{r}
trends_met |> 
  filter(model_error == TRUE) |> 
  select(station, parameter) |> 
  summarize(.by = station,
            parameters = paste(unique(parameter), collapse = "; ")) |> 
  arrange(station) |> 
  gt(caption = "MET stations where the model did not run")
```



***
***

# NUTs  

## trend calcs  

Attempting to use 12 knots for a seasonal smooth. This not possible at all stations (many are not sampled during winter) so at the ones where 12 knots causes a model error, I am allowing the model to choose the number of knots for me by setting it back to the default of k = -1. (This is all happening in the functions `run_bam_wq()` and `run_bam_nut()`).  

```{r}
# for testing
# nut_grid <- nut_grid[1:3, ]

# set up outputs
trends_out <- list()

# narncnut no23 testing
# i = 371

# hudtnnut too short time series for no23
# i = 350

# lksolwq too short time series for winter trends


# run the loop
for(i in 1:nrow(nut_grid)){
  stn <- as.character(nut_grid$station[i])
  param <- as.character(nut_grid$param[i])
  
  # subset to that stn/param combo
  tmp <- subset_df("nut", dat_nut, stn, param)
  
  # capture some info about the station and time series:
  ts_chars <- data.frame(ts_start = min(tmp$dec_date, na.rm = TRUE),
                         ts_end = max(tmp$dec_date, na.rm = TRUE),
                         ts_length = max(tmp$dec_date, na.rm = TRUE) - min(tmp$dec_date, na.rm = TRUE),
                         n_points = sum(!is.na(tmp$value)),
                         overall_median = median(tmp$value, na.rm = TRUE))
  
  
  # if the below statement is true, there was an error in the model
  # and we need to do something, and move to the next item in the loop
  # if it was false, we can move on
  if(inherits(try(run_bam_nut(tmp, k = 12), silent = TRUE), "try-error")){
    
    # pick our own k:
    knew <- tmp |> 
      summarize(.by = month,
                nVals = sum(!is.na(value))) |> 
      filter(nVals >= 1) |> 
      nrow()
    
    if(inherits(try(run_bam_nut(tmp, k = knew), silent = TRUE), "try-error")){
      # if that didn't work, make a blank row in the output:
      out <- data.frame(station = stn,
                        parameter = param,
                        model_error = TRUE)  
      # if it did work:
    } else {
      bam_out <- run_bam_nut(tmp, k = knew)
    }
    
    
  } else {
    # run model
    bam_out <- run_bam_nut(tmp, k = 12)
  }
  
  
  # even if it worked, if the time series is too short, remove it
  if(ts_chars$ts_length < 9){
    out <- data.frame(station = stn,
                      parameter = param,
                      model_error = TRUE)       
    if(exists("bam_out")){
      rm(bam_out)
    }
  }

  # do all the things to whichever bam_out we got
  # assuming we got one
  
  if(exists("bam_out")){
    
    # save the entire bam object, including autocorrelation info
    file_out <- here::here("Outputs", "calculated_trends", "bam_outputs",
                           paste0(stn, "_", param, "_bam.RData"))
    save(bam_out, file = file_out)
    
    out <- tidy_bam_output(bam_obj = bam_out)
    rm(bam_out)
  }
  
  out <- bind_cols(out, ts_chars)
  
  
  # break out trends by season
  # only if there's enough data overall
  if(ts_chars$ts_length >= 9){
    
    tmp <- left_join(tmp, seas_defs)
    season_vec <- unique(tmp$season)
    seasonals <- list()
    
    # for each season, make sure there are at least 7 distinct years
    # (this is somewhat arbitrary but there could be valid reasons a couple years get missed)
    for(j in seq_along(season_vec)){
      tmp_seas <- filter(tmp, season == season_vec[j])
      
      # if plain gam for a season works, run it
      if(!inherits(try(gam(lognut_mat ~ dec_date, family = cnorm(), data = tmp_seas),
                       silent = TRUE), "try-error")){
        
        mod <- gam(lognut_mat ~ dec_date,
                   family = cnorm(),
                   data = tmp_seas)
        seas_tbl <- gtsummary::tidy_gam(mod) |> 
          filter(term == "dec_date") |> 
          mutate(station = stn,
                 parameter= param,
                 season = season_vec[j]) |> 
          select(-term, -statistic, -parametric) |> 
          relocate(station, parameter, season)
        
        seasonals[[j]] <- seas_tbl
        
      } else {
        
        seasonals[[j]] <- data.frame(station = stn,
                                     parameter = param,
                                     season = season_vec[j],
                                     estimate = NA_real_,
                                     p.value = NA_real_)
      }
      
      
    }
    seasonal_trends <- bind_rows(seasonals) |> 
      pivot_wider(names_from = season,
                  values_from = c(estimate, p.value, std.error),
                  names_glue = "{season}_{.value}")
    
    out <- left_join(out, seasonal_trends)
    
  }
  
  trends_out[[i]] <- out
  rm(out)
}
```

```{r}
trends_nut <- dplyr::bind_rows(trends_out) |> 
  mutate(sig_trend = case_when(p.value <= 0.05 ~ "yes",
                               is.na(p.value) ~ NA_character_,
                               .default = "no"),
         sig_seasonality = case_when(Seas_p.val <= 0.05 ~ "yes",
                                     is.na(Seas_p.val) ~ NA_character_,
                                     .default = "no"),
         sig_autocorr = case_when(model_refit == TRUE ~ "yes",
                                  is.na(model_refit) ~ NA_character_,
                                  .default = "no"))
```


Any stations that didn't work?  


```{r}
trends_nut |> 
  filter(model_error == TRUE) |> 
  select(station, parameter) |> 
  summarize(.by = station,
            parameters = paste(unique(parameter), collapse = "; ")) |> 
  arrange(station) |> 
  gt(caption = "Nutrient stations where the model did not run")
```


# Save trend tables  

```{r}
save(trends_df, trends_met, trends_nut, file = here::here("Outputs", 
                                              "calculated_trends",
                                              "long-term-trends.RData"))

trends_all <- bind_rows(trends_df, trends_met, trends_nut)
write.csv(trends_all, file = here::here("Outputs",
                                        "calculated_trends",
                                        "long-term-trends.csv"))
```


# Session Info  

```{r}
devtools::session_info()

flnm <- paste0("session_info_Trends_", Sys.Date(), ".txt")
file_out <- here::here("R", "Long-Term_trend_analysis", flnm)
capture.output(devtools::session_info(), file = file_out)
```


    


