---
title: "Predictive Modeling"
format: html
---

# Setup  

```{r}
library(tidyverse)
library(MuMIn)
library(doParallel)
library(glmmTMB)
library(tictoc)
library(vegan)
library(ggfortify)
```


## Import predictors data frame  

```{r}
dat_all <- read.csv(here::here("Outputs", "compiled_predictors.csv"))
```



## PCA on temp/PAR/maybeLatitude  

```{r}
tpl <- dat_all |> 
  select(temp_median,
         dailyPAR_median,
         latitude)

pca_tpl <- prcomp(tpl, scale. = TRUE)
biplot(pca_tpl)
pca_tpl
summary(pca_tpl)
autoplot(pca_tpl,
         loadings = TRUE,
         loadings.label = TRUE)


pca_tp <- prcomp(tpl[, 1:2], scale. = TRUE)
biplot(pca_tp)
pca_tp
summary(pca_tp)
autoplot(pca_tp,
         loadings = TRUE,
         loadings.label = TRUE)
```



## Subset for response vars; center and scale predictors (not response)  

For now, only pulling water temp rather than running PCA - just trying to make sure model construction will work  

```{r}
dat_chl <- dat_all |> 
  select(reserve,
         chla_trend,
         # wq medians
         temp_median, spcond_median, turb_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median, nh4f_median, no23f_median, po4f_median,
         # nut trends
         nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) |> 
  mutate(across(temp_median:last_col(),
                function(x) as.vector(scale(x))))

formula_chl <- paste0("chla_trend ~ ", paste(names(dat_chl[3:ncol(dat_chl)]), collapse = " + "), " + (1|reserve)")
```


```{r}
dat_domgl <- dat_all |> 
  select(reserve,
         domgl_trend,
         # wq medians
         temp_median, spcond_median, turb_median, domgl_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median, nh4f_median, no23f_median, po4f_median,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) |> 
  mutate(across(temp_median:last_col(),
                function(x) as.vector(scale(x))))

formula_domgl <- paste0("domgl_trend ~ ", paste(names(dat_domgl[3:ncol(dat_domgl)]), collapse = " + "), " + (1|reserve)")

# generate formula without random effect
# due to singular fit when including random effect
formula_fixed_domgl <- paste0("domgl_trend ~ ", paste(names(dat_domgl[3:ncol(dat_domgl)]), collapse = " + "))
```


```{r}
dat_dopct <- dat_all |> 
  select(reserve,
         dopct_trend,
         # wq medians
         temp_median, spcond_median, turb_median, dopct_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median, nh4f_median, no23f_median, po4f_median,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) |> 
  mutate(across(temp_median:last_col(),
                function(x) as.vector(scale(x))))

formula_dopct <- paste0("dopct_trend ~ ", paste(names(dat_dopct[3:ncol(dat_dopct)]), collapse = " + "), " + (1|reserve)")
```



Don't have do<2 in the data frame yet.... need to revise something above  

```{r}
dat_doLT2 <- dat_all |> 
  select(reserve,
         domgl_trend,
         # wq medians
         temp_median, spcond_median, turb_median, domgl_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median, nh4f_median, no23f_median, po4f_median,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) |> 
  mutate(across(temp_median:last_col(),
                function(x) as.vector(scale(x))))

formula_doLT2 <- paste0("doLT2_trend ~ ", paste(names(dat_doLT2[3:ncol(dat_doLT2)]), collapse = " + "), " + (1|reserve)")
```

# Model  

## Create global models    

### Chl  

```{r}
mod_chl <- glmmTMB(as.formula(formula_chl),
                    data = dat_chl)
performance::check_singularity(mod_chl)
r.squaredGLMM(mod_chl)
summary(mod_chl)
```

### DO mgl

```{r}
mod_domgl <- glmmTMB(as.formula(formula_domgl),
                    data = dat_domgl)
performance::check_singularity(mod_domgl)
r.squaredGLMM(mod_domgl)
summary(mod_domgl)
```

### DO %

```{r}
mod_dopct <- glmmTMB(as.formula(formula_dopct),
                    data = dat_dopct)
performance::check_singularity(mod_dopct)
r.squaredGLMM(mod_dopct)
summary(mod_dopct)
```

### DO < 2  

```{r}
mod_doLT2 <- glmmTMB(as.formula(formula_doLT2),
                    data = dat_doLT2)
performance::check_singularity(mod_doLT2)
r.squaredGLMM(mod_doLT2)
summary(mod_doLT2)
```





## Dredging  

** everything below has eval: FALSE for the chunk** 

How many models will this be? How long does it take MuMIn to come up with the list, using multiple cores?  

16 predictors: 15 + r.e. of reserve.  


```{r}
#| eval: FALSE
#| 
# can subset for testing the dredging stuff below
dat_test <- dat_chl
formula <- paste0("chla_trend ~ ", paste(names(dat_test[3:ncol(dat_test)]), collapse = " + "), " + (1|reserve)")
mod_test <- glmmTMB(as.formula(formula),
                    data = dat_test)
r.squaredGLMM(mod_test)
```

 

```{r}
#| eval: FALSE
#| 
# establish cluster
cl <- makeCluster(10)  
registerDoParallel(cl)
```


It seems to be keeping the random effect in all models; I don't have to force it.  

```{r}
#| eval: FALSE
#| 
# name models, using cluster, and time it
options(na.action = "na.fail")

tic("identify models")
mod_subsets <- MuMIn::dredge(mod_test, eval = FALSE,
                             cluster = cl)
toc()

# examine
tic("examine models")
mod_subsets2 <- lapply(mod_subsets, as.character) |> 
  bind_rows()
mod_subsets2 <- mod_subsets2[2, ] |> 
  pivot_longer(1:ncol(mod_subsets2),
               names_to = "model",
               values_to = "call")
toc()
```

```{r}
#| eval: FALSE
#| 
# run models  
tic("run models")
mod_subsets <- MuMIn::dredge(mod_test, eval = TRUE,
                             cluster = cl)
toc()
beepr::beep(8)

# not using cluster for this call
# times below are for # of predictors in addition to the random effect
# 6 predictors: 14.72 seconds
# 10 predictors: 4.4 minutes
# if this holds, 17 predictors should take ~9.5 hours  
# 6 predictors a second time was ~16 seconds
# 10 predictors a second time: 4.4 minutes

# even though it says it's not using the cluster, this step still runs
# almost twice as fast when I include it as an argument
# (at least for 6 predictors - 29s vs. 15-16s)  
```

object size when eval=TRUE for 6 predictors was 120 bytes
for 10 predictors, also just 120 bytes

[if it doubles like the # of models do, we'll have 1920 bytes for 10 and
122,880 for 16. 122MB - RAM should be ablet to handle that]


```{r}
#| eval: FALSE
#| 
mods_best <- filter(mod_subsets,
                    delta <= 2)

top.mods <- get.models(mod_subsets, subset = delta<2)
avgd <- model.avg(top.mods)
avgd$coefficients

summary(avgd) # gives standard errors etc.
# Grueber et al. say it's best to report unconditional SE "as it incorporates model selection uncertainty, as opposed to standard SE which only considers sampling variance" 

# importances
sw(avgd)
```

"Full" coefficient is the 'zero method' in Grueber et al (coefficient shrinkage); 'subset' is 'natural average' method (does not create shrinkage).  


```{r}
#| eval: FALSE
#| 
tic("entire model")
dat_test <- dat_chl
formula <- paste0("chla_trend ~ ", paste(names(dat_test[3:ncol(dat_test)]), collapse = " + "), " + (1|reserve)")
mod_test <- glmmTMB(as.formula(formula),
                    data = dat_test)

tic("identify models")
mod_subsets <- MuMIn::dredge(mod_test, eval = FALSE,
                             cluster = cl)
toc()

# examine
tic("examine models")
mod_subsets2 <- lapply(mod_subsets, as.character) |> 
  bind_rows()
mod_subsets2 <- mod_subsets2[2, ] |> 
  pivot_longer(1:ncol(mod_subsets2),
               names_to = "model",
               values_to = "call")
toc()
toc()
```

For all 16 predictor vars, took 33 seconds to identify models and 3 seconds to pivot to be able to examine them. There are 65,536 models. Should take ~5 hrs to run them all.   

```{r}
#| eval: FALSE
#| 
# turn off cluster
stopCluster(cl)
```

