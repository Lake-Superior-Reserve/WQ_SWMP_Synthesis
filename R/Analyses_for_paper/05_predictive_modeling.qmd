---
title: "Predictive Modeling"
format: 
  html:
    toc: true
    code-fold: true
warning: false
message: false
error: true
embed-resources: true

---

# Setup  

```{r}
library(tidyverse)
library(MuMIn)
library(doParallel)
library(glmmTMB)
library(tictoc)
library(vegan)
library(ggfortify)
```


## Import predictors data frame  

```{r}
dat_all <- read.csv(here::here("Outputs", "compiled_predictors.csv"))
preds_doLT2 <- read.csv(here::here("Outputs", "doLT2_compiled_predictors.csv"))
```



## PCA on temp/PAR/maybeLatitude  

```{r}
tpl <- dat_all |> 
  select(temp_median,
         dailyPAR_median,
         latitude)

pca_tpl <- prcomp(tpl, scale. = TRUE)
biplot(pca_tpl)
pca_tpl
summary(pca_tpl)
autoplot(pca_tpl,
         loadings = TRUE,
         loadings.label = TRUE)


pca_tp <- prcomp(tpl[, 1:2], scale. = TRUE)
biplot(pca_tp)
pca_tp
summary(pca_tp)
autoplot(pca_tp,
         loadings = TRUE,
         loadings.label = TRUE)
```



## Subset for response vars; transform; center and scale predictors (not response)  

For now, only pulling water temp rather than running PCA - just trying to make sure model construction will work  

Natural-log transforming nutrient medians and turbidity, to ensure linearity of effects and stabilize variance.  

Centering and standardizing (1SD) predictors to make sure model converges and aid in relative interpretation (comparing effect sizes among variables on very different scales); once we have a top model set/averaged model, we can back-transform and interpret slopes in their raw units.  


```{r}
# in previous modeling, log-transformed nutrient and turb medians only
# had made histograms at some point
dat_all2 <- dat_all |> 
  mutate(across(c(chla_median,
                  nh4f_median,
                  no23f_median,
                  po4f_median,
                  turb_median),
                function(x) log(x))) |> 
  rename(turb_median.log = turb_median,
         chla_median.log = chla_median,
         nh4_median.log = nh4f_median,
         no23_median.log = no23f_median,
         po4_median.log = po4f_median)



preds_doLT2 <- preds_doLT2 |> 
  mutate(across(c(chla_median,
                  nh4f_median,
                  no23f_median,
                  po4f_median,
                  turb_median),
                function(x) log(x))) |> 
  rename(turb_median.log = turb_median,
         chla_median.log = chla_median,
         nh4_median.log = nh4f_median,
         no23_median.log = no23f_median,
         po4_median.log = po4f_median)
```

```{r}
#| eval: false

windows()

dat_all |> 
  select(station, do_pct_trend:last_col()) |> 
  pivot_longer(-station,
               names_to = "param",
               values_to = "value") |> 
  ggplot(aes(x = value,
             fill = param)) +
  geom_histogram(bins = 30,
                 col = "gray40") +
  facet_wrap(~param, scales = "free")


dat_all2 |> 
  select(station, do_pct_trend:last_col()) |> 
  pivot_longer(-station,
               names_to = "param",
               values_to = "value") |> 
  ggplot(aes(x = value,
             fill = param)) +
  geom_histogram(bins = 30,
                 col = "gray40") +
  facet_wrap(~param, scales = "free")

# transformed vars look much better
```

```{r}
dat_all <- dat_all2
```



```{r}
dat_chl <- dat_all |> 
  select(reserve,
         chla_trend,
         # wq medians
         temp_median, spcond_median, turb_median.log,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median.log, nh4_median.log, no23_median.log, po4_median.log,
         # nut trends
         nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) |> 
  mutate(across(temp_median:last_col(),
                function(x) as.vector(scale(x))))

formula_chl <- paste0("chla_trend ~ ", paste(names(dat_chl[3:ncol(dat_chl)]), collapse = " + "), " + (1|reserve)")
```


```{r}
dat_domgl <- dat_all |> 
  select(reserve,
         domgl_trend,
         # wq medians
         temp_median, spcond_median, turb_median.log, domgl_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median.log, nh4_median.log, no23_median.log, po4_median.log,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) |> 
  mutate(across(temp_median:last_col(),
                function(x) as.vector(scale(x))))

formula_domgl <- paste0("domgl_trend ~ ", paste(names(dat_domgl[3:ncol(dat_domgl)]), collapse = " + "), " + (1|reserve)")

# generate formula without random effect
# due to singular fit when including random effect
formula_fixed_domgl <- paste0("domgl_trend ~ ", paste(names(dat_domgl[3:ncol(dat_domgl)]), collapse = " + "))
```


```{r}
dat_dopct <- dat_all |> 
  select(reserve,
         dopct_trend = do_pct_trend,
         # wq medians
         temp_median, spcond_median, turb_median.log, dopct_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median.log, nh4_median.log, no23_median.log, po4_median.log,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) |> 
  mutate(across(temp_median:last_col(),
                function(x) as.vector(scale(x))))

formula_dopct <- paste0("dopct_trend ~ ", paste(names(dat_dopct[3:ncol(dat_dopct)]), collapse = " + "), " + (1|reserve)")

formula_fixed_dopct <- paste0("dopct_trend ~ ", paste(names(dat_dopct[3:ncol(dat_dopct)]), collapse = " + "))
```


```{r}
dat_doLT2 <- preds_doLT2 |> 
  select(reserve,
         doLT2_trend = do_below2_trend,
         # wq medians
         temp_median, spcond_median, turb_median.log, domgl_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median.log, nh4_median.log, no23_median.log, po4_median.log,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) |> 
  mutate(across(temp_median:last_col(),
                function(x) as.vector(scale(x))))

formula_doLT2 <- paste0("doLT2_trend ~ ", paste(names(dat_doLT2[3:ncol(dat_doLT2)]), collapse = " + "), " + (1|reserve)")

formula_fixed_doLT2 <- paste0("doLT2_trend ~ ", paste(names(dat_doLT2[3:ncol(dat_doLT2)]), collapse = " + "))
```

# Models - centered and scaled    

## Create global models    

### Chl  

```{r}
mod_chl <- glmmTMB(as.formula(formula_chl),
                    data = dat_chl)
performance::check_singularity(mod_chl)
r.squaredGLMM(mod_chl)
summary(mod_chl)
```

Marginal r2 = 0.2877  
Cond'l r2 = 0.7498  


### DO mgl

```{r}
mod_domgl <- glmmTMB(as.formula(formula_domgl),
                    data = dat_domgl)
performance::check_singularity(mod_domgl)


# drop random factor due to singular fit
mod_domgl <- lm(as.formula(formula_fixed_domgl),
                data = dat_domgl)
performance::check_singularity(mod_domgl)

summary(mod_domgl)$r.squared; summary(mod_domgl)$adj.r.squared
summary(mod_domgl)
```

### DO %

```{r}
mod_dopct <- glmmTMB(as.formula(formula_dopct),
                    data = dat_dopct)
performance::check_singularity(mod_dopct)

# drop random factor due to singular fit
mod_dopct <- lm(as.formula(formula_fixed_dopct),
                data = dat_dopct)
performance::check_singularity(mod_dopct)

summary(mod_dopct)$r.squared; summary(mod_dopct)$adj.r.squared
summary(mod_dopct)
```

### DO < 2  

```{r}
mod_doLT2 <- glmmTMB(as.formula(formula_doLT2),
                    data = dat_doLT2)
performance::check_singularity(mod_doLT2)

# drop random factor due to singular fit
mod_doLT2 <- lm(as.formula(formula_fixed_doLT2),
                data = dat_doLT2)
performance::check_singularity(mod_doLT2)

summary(mod_doLT2)$r.squared; summary(mod_doLT2)$adj.r.squared
summary(mod_doLT2)
```


# Un-scaled modeling  


## Subset for response vars  

For now, only pulling water temp rather than running PCA - just trying to make sure model construction will work  

```{r}
dat_chl <- dat_all |> 
  select(reserve,
         chla_trend,
         # wq medians
         temp_median, spcond_median, turb_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median, nh4f_median, no23f_median, po4f_median,
         # nut trends
         nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend)

formula_chl <- paste0("chla_trend ~ ", paste(names(dat_chl[3:ncol(dat_chl)]), collapse = " + "), " + (1|reserve)")
```


```{r}
dat_domgl <- dat_all |> 
  select(reserve,
         domgl_trend,
         # wq medians
         temp_median, spcond_median, turb_median, domgl_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median, nh4f_median, no23f_median, po4f_median,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) 

formula_domgl <- paste0("domgl_trend ~ ", paste(names(dat_domgl[3:ncol(dat_domgl)]), collapse = " + "), " + (1|reserve)")

# generate formula without random effect
# due to singular fit when including random effect
formula_fixed_domgl <- paste0("domgl_trend ~ ", paste(names(dat_domgl[3:ncol(dat_domgl)]), collapse = " + "))
```


```{r}
dat_dopct <- dat_all |> 
  select(reserve,
         dopct_trend = do_pct_trend,
         # wq medians
         temp_median, spcond_median, turb_median, dopct_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median, nh4f_median, no23f_median, po4f_median,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend) 

formula_dopct <- paste0("dopct_trend ~ ", paste(names(dat_dopct[3:ncol(dat_dopct)]), collapse = " + "), " + (1|reserve)")

formula_fixed_dopct <- paste0("dopct_trend ~ ", paste(names(dat_dopct[3:ncol(dat_dopct)]), collapse = " + "))
```


```{r}
dat_doLT2 <- preds_doLT2 |> 
  select(reserve,
         doLT2_trend = do_below2_trend,
         # wq medians
         temp_median, spcond_median, turb_median, domgl_median,
         # wq trends
         temp_trend, spcond_trend, turb_trend,
         # nut medians
         chla_median, nh4f_median, no23f_median, po4f_median,
         # nut trends
         chla_trend, nh4f_trend, no23f_trend, po4f_trend,
         # met
         precp_median, precp_trend, dailyPAR_trend)

formula_doLT2 <- paste0("doLT2_trend ~ ", paste(names(dat_doLT2[3:ncol(dat_doLT2)]), collapse = " + "), " + (1|reserve)")

formula_fixed_doLT2 <- paste0("doLT2_trend ~ ", paste(names(dat_doLT2[3:ncol(dat_doLT2)]), collapse = " + "))
```


## Create global models    

### Chl  

```{r}
mod_chl <- glmmTMB(as.formula(formula_chl),
                    data = dat_chl)
performance::check_singularity(mod_chl)
r.squaredGLMM(mod_chl)
summary(mod_chl)
```

Marginal r2 = 0.2877  
Cond'l r2 = 0.7498 

centering/scaling didn't change..... interesting  


### DO mgl

```{r}
mod_domgl <- glmmTMB(as.formula(formula_domgl),
                    data = dat_domgl)
performance::check_singularity(mod_domgl)


# drop random factor due to singular fit
mod_domgl <- lm(as.formula(formula_fixed_domgl),
                data = dat_domgl)
performance::check_singularity(mod_domgl)

summary(mod_domgl)$r.squared; summary(mod_domgl)$adj.r.squared
summary(mod_domgl)
```

### DO %

```{r}
mod_dopct <- glmmTMB(as.formula(formula_dopct),
                    data = dat_dopct)
performance::check_singularity(mod_dopct)

# drop random factor due to singular fit
mod_dopct <- lm(as.formula(formula_fixed_dopct),
                data = dat_dopct)
performance::check_singularity(mod_dopct)

summary(mod_dopct)$r.squared; summary(mod_dopct)$adj.r.squared
summary(mod_dopct)
```

### DO < 2  

```{r}
mod_doLT2 <- glmmTMB(as.formula(formula_doLT2),
                    data = dat_doLT2)
performance::check_singularity(mod_doLT2)

# drop random factor due to singular fit
mod_doLT2 <- lm(as.formula(formula_fixed_doLT2),
                data = dat_doLT2)
performance::check_singularity(mod_doLT2)

summary(mod_doLT2)$r.squared; summary(mod_doLT2)$adj.r.squared
summary(mod_doLT2)
```









# Dredging  



## Dredging  

** everything below has eval: FALSE for the chunk** 

How many models will this be? How long does it take MuMIn to come up with the list, using multiple cores?  

16 predictors: 15 + r.e. of reserve.  


```{r}
#| eval: FALSE
#| 
# can subset for testing the dredging stuff below
dat_test <- dat_chl
formula <- paste0("chla_trend ~ ", paste(names(dat_test[3:ncol(dat_test)]), collapse = " + "), " + (1|reserve)")
mod_test <- glmmTMB(as.formula(formula),
                    data = dat_test)
r.squaredGLMM(mod_test)
```

 

```{r}
#| eval: FALSE
#| 
# establish cluster
cl <- makeCluster(10)  
registerDoParallel(cl)
```


It seems to be keeping the random effect in all models; I don't have to force it.  

```{r}
#| eval: FALSE
#| 
# name models, using cluster, and time it
options(na.action = "na.fail")

tic("identify models")
mod_subsets <- MuMIn::dredge(mod_test, eval = FALSE,
                             cluster = cl)
toc()

# examine
tic("examine models")
mod_subsets2 <- lapply(mod_subsets, as.character) |> 
  bind_rows()
mod_subsets2 <- mod_subsets2[2, ] |> 
  pivot_longer(1:ncol(mod_subsets2),
               names_to = "model",
               values_to = "call")
toc()
```

```{r}
#| eval: FALSE
#| 
# run models  
tic("run models")
mod_subsets <- MuMIn::dredge(mod_test, eval = TRUE,
                             cluster = cl)
toc()
beepr::beep(8)

# not using cluster for this call
# times below are for # of predictors in addition to the random effect
# 6 predictors: 14.72 seconds
# 10 predictors: 4.4 minutes
# if this holds, 17 predictors should take ~9.5 hours  
# 6 predictors a second time was ~16 seconds
# 10 predictors a second time: 4.4 minutes

# even though it says it's not using the cluster, this step still runs
# almost twice as fast when I include it as an argument
# (at least for 6 predictors - 29s vs. 15-16s)  
```

object size when eval=TRUE for 6 predictors was 120 bytes
for 10 predictors, also just 120 bytes

[if it doubles like the # of models do, we'll have 1920 bytes for 10 and
122,880 for 16. 122MB - RAM should be ablet to handle that]


```{r}
#| eval: FALSE
#| 
mods_best <- filter(mod_subsets,
                    delta <= 2)

top.mods <- get.models(mod_subsets, subset = delta<2)
avgd <- model.avg(top.mods)
avgd$coefficients

summary(avgd) # gives standard errors etc.
# Grueber et al. say it's best to report unconditional SE "as it incorporates model selection uncertainty, as opposed to standard SE which only considers sampling variance" 

# importances
sw(avgd)
```

"Full" coefficient is the 'zero method' in Grueber et al (coefficient shrinkage); 'subset' is 'natural average' method (does not create shrinkage).  


```{r}
#| eval: FALSE
#| 
tic("entire model")
dat_test <- dat_chl
formula <- paste0("chla_trend ~ ", paste(names(dat_test[3:ncol(dat_test)]), collapse = " + "), " + (1|reserve)")
mod_test <- glmmTMB(as.formula(formula),
                    data = dat_test)

tic("identify models")
mod_subsets <- MuMIn::dredge(mod_test, eval = FALSE,
                             cluster = cl)
toc()

# examine
tic("examine models")
mod_subsets2 <- lapply(mod_subsets, as.character) |> 
  bind_rows()
mod_subsets2 <- mod_subsets2[2, ] |> 
  pivot_longer(1:ncol(mod_subsets2),
               names_to = "model",
               values_to = "call")
toc()
toc()
```

For all 16 predictor vars, took 33 seconds to identify models and 3 seconds to pivot to be able to examine them. There are 65,536 models. Should take ~5 hrs to run them all.   

```{r}
#| eval: FALSE
#| 
# turn off cluster
stopCluster(cl)
```

